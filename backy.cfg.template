[DEFAULTS]
# Where should the backy logfile be placed?
# Backy will by default log INFO, WARNING and ERROR to this log.
# If you also need DEBUG information, please start backy with '-v'.
logfile: /var/log/backy.log

# Default block size. 4MB are recommended.
# DO NOT CHANGE WHEN BACKUPS EXIST!
block_size: 4194304

# Hash function to use. Use a large one to avoid collisions.
# DO NOT CHANGE WHEN BACKUPS EXIST!
hash_function: sha512

# for some operations, full backy or single versions need to be locked for
# simulatanous access. In the lock_dir we will create backy_*.lock files.
lock_dir: /run

# To be able to find other backys running, we need a system-wide unique name
# for all backy processes.
# DO NOT CHANGE WHILE backy2 PROCESSES ARE RUNNING!
process_name: backy2

# Allow rm of backup versions after n days (set to 0 to disable, i.e. to be
# able to delete any version)
disallow_rm_when_younger_than_days: ${PROTECT_YOUNG_BACKUP_DAYS}


[MetaBackend]
# Of which type is the Metadata Backend Engine?
# Available types:
#   backy2.meta_backends.sql

#######################################
# backy2.meta_backends.sql
#######################################
type: backy2.meta_backends.sql

# Which SQL Server?
# Available servers:
#   sqlite:////path/to/sqlitefile
#   postgresql:///database
#   postgresql://user:password@host:port/database
engine: ${INDEX_DB_ADDRESS}


[DataBackend]
# Which data backend to use?
# Available types:
#   backy2.data_backends.file
#   backy2.data_backends.s3
#   backy2.data_backends.azureblob

type: backy2.data_backends.${TARGET_DATA_BACKEND}

#######################################
# backy2.data_backends.file
#######################################

# Store data to this path. A structure of 2 folders depth will be created
# in this path (e.g. '0a/33'). Blocks of DEFAULTS.block_size will be stored
# there. This is your backup storage!
path: /var/lib/backy2/data

# How many writes to perform in parallel. This is useful if your backup space
# can perform parallel writes faster than serial ones.
# simultaneous_writes: ${SIMULTANEOUS_WRITES}

# How many reads to perform in parallel. This is useful if your backup space
# can perform parallel reads faster than serial ones.
# simultaneous_reads: ${SIMULTANEOUS_READS}

# Bandwidth throttling (set to 0 to disable, i.e. use full bandwidth)
# bytes per second
# bandwidth_read: ${MAX_BANDWIDTH_READ}
# bandwidth_write: ${MAX_BANDWIDTH_WRITE}


#######################################
# backy2.data_backends.azureblob
#######################################
# type: backy2.data_backends.azureblob

# Your azure blob access key
azure_access_key_id: ${AZURE_ACCESS_KEY_ID}

# Your azure blob secret access key
azure_secret_access_key: ${AZURE_SECRET_ACCESS_KEY}

# Store to this container name:
bucket_name: ${AZURE_BUCKET_NAME}

# How many azure blob puts to perform in parallel
simultaneous_writes: ${SIMULTANEOUS_WRITES}

# How many reads to perform in parallel. This is useful if your backup space
# can perform parallel reads faster than serial ones.
simultaneous_reads: ${SIMULTANEOUS_READS}

# Bandwidth throttling (set to 0 to disable, i.e. use full bandwidth)
# bytes per second
bandwidth_read: ${MAX_BANDWIDTH_READ}
bandwidth_write: ${MAX_BANDWIDTH_WRITE}


#######################################
# backy2.data_backends.s3
#######################################
#type: backy2.data_backends.s3

# Your s3 access key
# aws_access_key_id: ${S3_AWS_ACCESS_KEY_ID}

# Your s3 secret access key
# aws_secret_access_key: ${S3_AWS_SECRET_ACCESS_KEY}

# Your aws host (IP or name)
# host: ${S3_AWS_HOST}
# host: s3proxy

# The port to connect to (usually 80 if not secure or 443 if secure)
# port: ${S3_AWS_PORT}

# Use HTTPS?
# is_secure: ${S3_AWS_HTTPS}

# Store to this bucket name:
# bucket_name: ${S3_AWS_BUCKET_NAME}

# How many s3 puts to perform in parallel
# simultaneous_writes: 5

# How many reads to perform in parallel. This is useful if your backup space
# can perform parallel reads faster than serial ones.
# simultaneous_reads: 5

# Bandwidth throttling (set to 0 to disable, i.e. use full bandwidth)
# bytes per second
# bandwidth_read: 78643200
# bandwidth_write: 78643200


[NBD]
# Where to cache backy blocks when NBD Server is used
# Enterprise version only
cachedir: /tmp


[io_file]
# Configure the file IO (file://<path>)
# This is for a file or a blockdevice (e.g. /dev/sda)

# How many parallel reads are permitted? (also affects the queue length)
simultaneous_reads: ${SIMULTANEOUS_READS}


[io_rbd]
# Configure the rbd IO (rbd://<pool>/<imagename>[@<snapshotname>])
# This accepts rbd images in the form rbd://pool/image@snapshot or rbd://pool/image

# Where to look for the ceph configfile to read keys and hosts from it
ceph_conffile: /etc/ceph/ceph.conf

# How many parallel reads are permitted? (also affects the queue length)
simultaneous_reads: ${SIMULTANEOUS_READS}

# When restoring images, new images are created (if you don't --force). For these
# newly created images, use these features:
new_image_features:
    RBD_FEATURE_LAYERING
    RBD_FEATURE_EXCLUSIVE_LOCK
    RBD_FEATURE_STRIPINGV2
    RBD_FEATURE_OBJECT_MAP
    RBD_FEATURE_FAST_DIFF
    RBD_FEATURE_DEEP_FLATTEN

